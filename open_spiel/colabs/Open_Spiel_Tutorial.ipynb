{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Open Spiel Tutorial",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepmind/open_spiel/blob/master/open_spiel/colabs/Open_Spiel_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC6kQBzWahEF"
      },
      "source": [
        "## Install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56XXlv424Vf7"
      },
      "source": [
        "Install a recent clang compiler:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJi4i4QV3_W4"
      },
      "source": [
        "!apt-get install clang-9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2_Vbijh4FlZ"
      },
      "source": [
        "Build & install the pip source distribution of OpenSpiel:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQc12Xrn4CXU"
      },
      "source": [
        "!CXX=`which clang++-9` pip install --upgrade --verbose open_spiel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUtlXZ8FBnAL"
      },
      "source": [
        "# It's play time!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewMXCaUw8d9Q"
      },
      "source": [
        "import itertools as it\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pyspiel\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=True, floatmode='fixed')\n",
        "\n",
        "from open_spiel.python.algorithms import exploitability\n",
        "from open_spiel.python import policy as policy_lib\n",
        "\n",
        "game = pyspiel.load_game('tic_tac_toe')\n",
        "state = game.new_initial_state()\n",
        "\n",
        "while not state.is_terminal():\n",
        "  action = np.random.choice(state.legal_actions())\n",
        "  print(f'Taking action {action} {state.action_to_string(action)}')\n",
        "  state.apply_action(action)\n",
        "  print(state)\n",
        "print(f'Game over; returns {state.returns()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioNmWry_lbWA"
      },
      "source": [
        "game = pyspiel.load_game('kuhn_poker')\n",
        "print(game.get_type().pretty_print())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U6iy9YTlc-H"
      },
      "source": [
        "policy = policy_lib.TabularPolicy(game)\n",
        "print(policy.states_per_player)\n",
        "print(policy.action_probability_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwQBgGfXN83z"
      },
      "source": [
        "def print_policy(policy):\n",
        "  for state, probs in zip(it.chain(*policy.states_per_player),\n",
        "                          policy.action_probability_array):\n",
        "    print(f'{state:6}   p={probs}')\n",
        "\n",
        "print_policy(policy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRwbSW3dN86i"
      },
      "source": [
        "print(exploitability.nash_conv(game, policy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBAuDeNjN89E"
      },
      "source": [
        "def new_reach(so_far, player, action_prob):\n",
        "  \"\"\"Returns new reach probabilities.\"\"\"\n",
        "  new = np.array(so_far)\n",
        "  new[player] *= action_prob\n",
        "  return new\n",
        "\n",
        "def calc_cfr(state, reach):\n",
        "  \"\"\"Updates regrets; returns utility for all players.\"\"\"\n",
        "  if state.is_terminal():\n",
        "    return state.returns()\n",
        "  elif state.is_chance_node():\n",
        "    return sum(prob * calc_cfr(state.child(action), new_reach(reach, -1, prob))\n",
        "               for action, prob in state.chance_outcomes())\n",
        "  else:\n",
        "    # We are at a player decision point.\n",
        "    player = state.current_player()\n",
        "    index = policy.state_index(state)\n",
        "    \n",
        "    # Compute utilities after each action, updating regrets deeper in the tree.\n",
        "    utility = np.zeros((game.num_distinct_actions(), game.num_players()))\n",
        "    for action in state.legal_actions():\n",
        "      prob = curr_policy[index][action]\n",
        "      utility[action] = calc_cfr(state.child(action), new_reach(reach, player, prob))\n",
        "\n",
        "    # Compute regrets at this state.\n",
        "    cfr_prob = np.prod(reach[:player]) * np.prod(reach[player+1:])\n",
        "    value = np.einsum('ap,a->p', utility, curr_policy[index])\n",
        "    for action in state.legal_actions():\n",
        "      regrets[index][action] += cfr_prob * (utility[action][player] - value[player])\n",
        "\n",
        "    # Return the value of this state for all players.\n",
        "    return value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ygD_ReRN8_i"
      },
      "source": [
        "game = pyspiel.load_game('kuhn_poker')\n",
        "policy = policy_lib.TabularPolicy(game)\n",
        "initial_state = game.new_initial_state()\n",
        "curr_policy = policy.action_probability_array.copy()\n",
        "regrets = np.zeros_like(policy.action_probability_array)\n",
        "eval_steps = []\n",
        "eval_nash_conv = []\n",
        "for step in range(2049):\n",
        "  # Compute regrets\n",
        "  calc_cfr(initial_state, np.ones(1 + game.num_players()))\n",
        "\n",
        "  # Find the new regret-matching policy\n",
        "  floored_regrets = np.maximum(regrets, 1e-16)\n",
        "  sum_floored_regrets = np.sum(floored_regrets, axis=1, keepdims=True)\n",
        "  curr_policy = floored_regrets / sum_floored_regrets\n",
        "\n",
        "  # Update the average policy\n",
        "  lr = 1 / (1 + step)\n",
        "  policy.action_probability_array *= (1 - lr)\n",
        "  policy.action_probability_array += curr_policy * lr\n",
        "\n",
        "  # Evaluate the average policy\n",
        "  if step & (step-1) == 0:\n",
        "    nc = exploitability.nash_conv(game, policy)\n",
        "    eval_steps.append(step)\n",
        "    eval_nash_conv.append(nc)\n",
        "    print(f'Nash conv after step {step} is {nc}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG2H8KxtN9CK"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.set_title(\"NashConv by CFR Iteration\")\n",
        "ax.plot(eval_steps, eval_nash_conv)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN6ahXS1N9Eh"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.set_title(\"NashConv by CFR Iteration (log-log scale)\")\n",
        "ax.loglog(eval_steps, eval_nash_conv)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaT_Uc1LOGe3"
      },
      "source": [
        "# Display the whole policy\n",
        "print_policy(policy)\n",
        "\n",
        "# How likely are we to bet with a Jack?\n",
        "alpha = policy.action_probability_array[policy.state_lookup['0']][1]\n",
        "print(f'P(bet with Jack) = alpha = {alpha:.3}')\n",
        "\n",
        "# How likely are we to bet with a King?\n",
        "pK = policy.action_probability_array[policy.state_lookup['2']][1]\n",
        "print(f'P(bet with King) = {pK:.3}, cf {alpha * 3:.3}')\n",
        "\n",
        "# How likely are we to call with a Queen?\n",
        "pQ = policy.action_probability_array[policy.state_lookup['1pb']][1]\n",
        "print(f'P(call with Queen after checking) = {pQ:.3}, cf {alpha + 1/3:.3}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRy-dK1AOH7f"
      },
      "source": [
        "def sample(actions_and_probs):\n",
        "  actions, probs = zip(*actions_and_probs)\n",
        "  return np.random.choice(actions, p=probs)\n",
        "\n",
        "def policy_as_list(policy, state):\n",
        "  return list(enumerate(policy.policy_for_key(state.information_state_string())))\n",
        "\n",
        "def env_action(state):\n",
        "  if state.is_chance_node():\n",
        "    p = state.chance_outcomes()\n",
        "  else:\n",
        "    p = policy_as_list(fixed_policy, state)\n",
        "  return sample(p)\n",
        "\n",
        "def softmax(x):\n",
        "  x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "  return x / np.sum(x, axis=-1, keepdims=True)\n",
        "\n",
        "def generate_trajectory(state, player):\n",
        "  trajectory = []\n",
        "  while not state.is_terminal():\n",
        "    if state.current_player() == player:\n",
        "      action = sample(policy_as_list(rl_policy, state))\n",
        "      trajectory.append((rl_policy.state_index(state), action))\n",
        "    else:\n",
        "      action = env_action(state)\n",
        "    state.apply_action(action)\n",
        "  return trajectory, state.returns()[player]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIXlPgULOKlW"
      },
      "source": [
        "fixed_policy = policy_lib.TabularPolicy(game)\n",
        "rl_policy = policy_lib.TabularPolicy(game)\n",
        "for _ in range(5):\n",
        "  print(generate_trajectory(game.new_initial_state(), player=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zMhyvsoOMFJ"
      },
      "source": [
        "# Run REINFORCE\n",
        "N = 10000\n",
        "lr = 0.01\n",
        "for step in range(N):\n",
        "  for player in (0, 1):\n",
        "    trajectory, reward = generate_trajectory(game.new_initial_state(), player)\n",
        "    for s, a in trajectory:\n",
        "      logits = np.log(rl_policy.action_probability_array[s])\n",
        "      logits[a] += lr * reward\n",
        "      rl_policy.action_probability_array[s] = softmax(logits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3J_c_3xOO-L"
      },
      "source": [
        "# Evaluate the policy\n",
        "def evaluate(state, rl_policy, player):\n",
        "  if state.is_terminal():\n",
        "    return state.returns()[player]\n",
        "  elif state.current_player() == player:\n",
        "    ap = policy_as_list(rl_policy, state)\n",
        "  elif state.is_chance_node():\n",
        "    ap = state.chance_outcomes()\n",
        "  else:\n",
        "    ap = policy_as_list(fixed_policy, state)\n",
        "  return sum(p * evaluate(state.child(a), rl_policy, player) for a, p in ap)\n",
        "  \n",
        "def eval(rl_policy):\n",
        "  return (evaluate(game.new_initial_state(), rl_policy, player=0)\n",
        "        + evaluate(game.new_initial_state(), rl_policy, player=1))\n",
        "\n",
        "print_policy(rl_policy)  \n",
        "eval(rl_policy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vSHt1igORKq"
      },
      "source": [
        "# Evaluate the greedy policy\n",
        "greedy_policy = policy_lib.TabularPolicy(game)\n",
        "greedy_policy.action_probability_array = (np.eye(game.num_distinct_actions())\n",
        "              [np.argmax(rl_policy.action_probability_array, axis=-1)])\n",
        "\n",
        "print_policy(greedy_policy)\n",
        "eval(greedy_policy)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}